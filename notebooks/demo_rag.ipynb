{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aviation Engine Maintenance RAG Demo\n",
    "\n",
    "This demo shows how to:\n",
    "- Load FAA handbook PDFs\n",
    "- Create embeddings + FAISS index\n",
    "- Run retrieval-augmented QA with a local LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running standalone\n",
    "!pip install langchain sentence-transformers faiss-cpu pypdf transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "\n",
    "PDF_DIR = \"../data/pdfs\"\n",
    "\n",
    "# Collect all PDFs\n",
    "pdf_files = [os.path.join(PDF_DIR, f) for f in os.listdir(PDF_DIR) if f.endswith(\".pdf\")]\n",
    "len(pdf_files), pdf_files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split into text chunks\n",
    "docs = []\n",
    "for f in pdf_files:\n",
    "    loader = PyPDFLoader(f)\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embeddings + FAISS index\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a lightweight LLM pipeline (Gemma or LLaMA)\n",
    "model_id = \"google/gemma-2-2b-it\"  # or \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model_id, device_map=\"auto\", max_new_tokens=256)\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "\n",
    "query = \"What is the function of the ignition system in an aircraft engine?\"\n",
    "result = qa.run(query)\n",
    "print(\"Q:\", query)\n",
    "print(\"A:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
